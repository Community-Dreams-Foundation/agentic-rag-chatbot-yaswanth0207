# No API key needed â€” this project uses Ollama (local LLM).
# Make sure Ollama is running: ollama serve
# And that you have the model pulled: ollama pull llama3.2
